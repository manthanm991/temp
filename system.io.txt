<------------- IR 1 ------------->
Requirements:
nltk.download('punkt')
nltk.download('stopwords')

import nltk
text = "The weather today is much better than it was yesterday due to the clouds"
tokens = nltk.word_tokenize(text.lower())
stopwords = nltk.corpus.stopwords.words('english')
filtered_tokens = [word for word in tokens if word not in stopwords]
Lemmatizer = nltk.stem.WordNetLemmatizer()
stem_tokens = [Lemmatizer.lemmatize(word) for word in filtered_tokens]
print(tokens)
print(filtered_tokens)
print(stem_tokens)

<------------- IR 2 ------------->

documents = [
"Natural Language Processing is an exciting field of AI.",
"Document retrieval is a key concept in Information Retrieval.",
"AI and machine learning are closely related fields.",
"Natural Language Processing and Information Retrieval often overlap.",
"Machine learning plays a crucial role in AI."
]

inverted_index = {}
for doc_id, doc in enumerate(documents):
    words = doc.lower().split()
    for word in words:
        if word not in inverted_index:
            inverted_index[word] = [doc_id]
        elif doc_id not in inverted_index[word]:
            inverted_index[word].append(doc_id)

def retrieve_documents(query):
    query_words = query.lower().split()
    relevant_docs = set()
    for word in query_words:
        if word in inverted_index:
            if not relevant_docs:
                relevant_docs = set(inverted_index[word])
            else:
                relevant_docs = relevant_docs.intersection(inverted_index[word])
    return list(relevant_docs)

print("Inverted Index:")
for word, doc_ids in inverted_index.items():
    print(f"{word}: {doc_ids}")
query = "Natural Language Processing"
retrieved_doc_ids = retrieve_documents(query)
print("\nRetrieved Documents for the Query:", query)
if retrieved_doc_ids:
    for doc_id in retrieved_doc_ids:
        print(f"Document {doc_id + 1}: {documents[doc_id]}")
    else:
        print("No documents found for the given query.")

<------------- IR 3 ------------->
Requirements:
pip install scikit-learn pandas

import pandas as pd
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

categories = ['rec.autos', 'rec.motorcycles', 'sci.electronics', 'sci.med'] # Categories with spam-like data
newsgroups = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)

data = pd.DataFrame({'message': newsgroups.data, 'label': newsgroups.target})
print(data.head())

data['label'] = data['label'].apply(lambda x: 0 if x < 2 else 1)

X_train, X_test, y_train, y_test = train_test_split(data['message'], data['label'], test_size=0.3, random_state=42)

vectorizer = CountVectorizer(stop_words='english')
X_train_transformed = vectorizer.fit_transform(X_train)
X_test_transformed = vectorizer.transform(X_test)

model = MultinomialNB()
model.fit(X_train_transformed, y_train)
y_pred = model.predict(X_test_transformed)
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

<------------- IR 4 ------------->
Requirements:
pip install matplotlib seaborn

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

iris = load_iris()
X = iris.data
y = iris.target

df = pd.DataFrame(X, columns=iris.feature_names)

agglom = AgglomerativeClustering(n_clusters=3)
clusters = agglom.fit_predict(X)

df['Cluster'] = clusters

plt.figure(figsize=(10, 7))
sns.scatterplot(x=df[iris.feature_names[0]], y=df[iris.feature_names[1]], hue=df['Cluster'], palette='viridis', s=100)
plt.title('Agglomerative Hierarchical Clustering on Iris Dataset')
plt.xlabel(iris.feature_names[0])
plt.ylabel(iris.feature_names[1])
plt.legend(title='Cluster')
plt.grid(True)
plt.show()

plt.figure(figsize=(12, 8))
linked = linkage(X, 'ward')
dendrogram(linked, orientation='top', labels=iris.target_names[y], distance_sort='descending', show_leaf_counts=True)
plt.title('Dendrogram for Agglomerative Clustering')
plt.xlabel('Iris Species')
plt.ylabel('Distance')
plt.show()

<------------- IR 5 ------------->

import numpy as np
def pagerank(graph, num_iterations: int = 100, d: float = 0.85) -> np.array:
    n = graph.shape[0]
    pagerank_scores = np.ones(n) / n
    for _ in range(num_iterations):
        new_pagerank_scores = np.zeros(n)
        for i in range(n):
            for j in range(n):
                if graph[j][i] == 1:
                    new_pagerank_scores[i] += pagerank_scores[j] / np.sum(graph[j])
            new_pagerank_scores[i] = d * new_pagerank_scores[i] + (1 - d) / n
        pagerank_scores = new_pagerank_scores
    return pagerank_scores
if __name__ == "__main__":
    graph = np.array([[0, 1, 1, 0],
                      [0, 0, 1, 1],
                      [1, 0, 0, 1],
                      [1, 1, 0, 0]])
scores = pagerank(graph)
print("PageRank Scores:", scores)